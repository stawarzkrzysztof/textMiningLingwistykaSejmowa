{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c2b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Iterable, Optional\n",
    "\n",
    "import httpx\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623862d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/krzysztofstawarz/GithubRepositories/textMiningLingwistykaSejmowa/data/raw/term10'),\n",
       " PosixPath('/Users/krzysztofstawarz/GithubRepositories/textMiningLingwistykaSejmowa/data/tables/term10'),\n",
       " PosixPath('/Users/krzysztofstawarz/GithubRepositories/textMiningLingwistykaSejmowa/logs'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====== KONFIG (zmień tu) ======\n",
    "TERM = 10\n",
    "API_ROOT = \"https://api.sejm.gov.pl\"\n",
    "USER_AGENT = \"sejm-textmining/0.1 (research; contact: kstawarz@student.agh.edu.pl)\"  # możesz zmienić\n",
    "\n",
    "# Wydajność (Mac mini M4 spokojnie uciągnie wysoką współbieżność, ale nie katuj API bez sensu)\n",
    "MAX_CONCURRENCY = 60          # typowo 40–80 jest OK\n",
    "MAX_CONNECTIONS = 200\n",
    "MAX_KEEPALIVE = 50\n",
    "\n",
    "# Paginacja (ile rekordów na stronę dla list interpelacji / zapytań)\n",
    "PAGE_LIMIT = 50              # dokumentacja: domyślnie często 50; 100 zwykle działa\n",
    "\n",
    "# Co pobierać:\n",
    "DOWNLOAD_TRANSCRIPTS = True\n",
    "DOWNLOAD_INTERPELLATIONS = True\n",
    "DOWNLOAD_WRITTEN_QUESTIONS = True\n",
    "DOWNLOAD_COMMITTEE_SITTINGS = False   # może być bardzo dużo danych\n",
    "\n",
    "# Załączniki (PDF itp.)\n",
    "DOWNLOAD_ATTACHMENTS = True\n",
    "\n",
    "# Katalog projektu: uruchamiaj notebook z root repo (tam gdzie masz np. data/)\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\" / f\"term{TERM}\"\n",
    "TABLES_DIR = DATA_DIR / \"tables\" / f\"term{TERM}\"\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "LOG_PATH = LOGS_DIR / f\"sejm_term{TERM}_{RUN_TAG}.log\"\n",
    "\n",
    "RAW_DIR, TABLES_DIR, LOGS_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aca61f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: /Users/krzysztofstawarz/GithubRepositories/textMiningLingwistykaSejmowa/data/raw/term10\n"
     ]
    }
   ],
   "source": [
    "def ensure_dirs() -> None:\n",
    "    # MP\n",
    "    (RAW_DIR / \"mp\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"mp\" / \"details\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Clubs\n",
    "    (RAW_DIR / \"clubs\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"clubs\" / \"details\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Proceedings + transcripts\n",
    "    (RAW_DIR / \"proceedings\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"proceedings\" / \"details\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"transcripts\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"transcripts\" / \"index_by_day\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"transcripts\" / \"html_by_mp\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"transcripts\" / \"html_non_mp\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Interpellations\n",
    "    (RAW_DIR / \"interpellations\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"interpellations\" / \"pages\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"interpellations\" / \"by_num\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"interpellations\" / \"by_author_ptr\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Written questions\n",
    "    (RAW_DIR / \"writtenQuestions\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"writtenQuestions\" / \"pages\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"writtenQuestions\" / \"by_num\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"writtenQuestions\" / \"by_author_ptr\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Committees\n",
    "    (RAW_DIR / \"committees\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"committees\" / \"details\").mkdir(parents=True, exist_ok=True)\n",
    "    (RAW_DIR / \"committees\" / \"sittings\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Tables\n",
    "    TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ensure_dirs()\n",
    "print(\"OK:\", RAW_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3c5d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 14:50:26,585 | INFO | Log file: /Users/krzysztofstawarz/GithubRepositories/textMiningLingwistykaSejmowa/logs/sejm_term10_20260111_145026.log\n"
     ]
    }
   ],
   "source": [
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_PATH, encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(\"sejm\")\n",
    "logger.info(\"Log file: %s\", LOG_PATH)\n",
    "_slug_re = re.compile(r\"[^A-Za-z0-9]+\")\n",
    "\n",
    "def slugify(s: str, max_len: int = 80) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return \"unknown\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    s = _slug_re.sub(\"-\", s).strip(\"-\").lower()\n",
    "    return (s[:max_len] if s else \"unknown\")\n",
    "\n",
    "def safe_int(x: Any) -> Optional[int]:\n",
    "    try:\n",
    "        if x is None:\n",
    "            return None\n",
    "        if isinstance(x, int):\n",
    "            return x\n",
    "        s = str(x).strip()\n",
    "        if not s:\n",
    "            return None\n",
    "        if re.fullmatch(r\"\\d+\", s):\n",
    "            return int(s)\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "async def write_bytes(path: Path, data: bytes) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    await asyncio.to_thread(path.write_bytes, data)\n",
    "\n",
    "async def write_text(path: Path, text: str) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    await asyncio.to_thread(path.write_text, text, encoding=\"utf-8\")\n",
    "\n",
    "async def write_json(path: Path, obj: Any) -> None:\n",
    "    text = json.dumps(obj, ensure_ascii=False, indent=2)\n",
    "    await write_text(path, text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b4c36c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     77\u001b[39m                     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     79\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m mp_list, mp_details, clubs_list\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m mp_list, mp_details, clubs_list = \u001b[38;5;28;01mawait\u001b[39;00m download_mps_and_clubs()\n\u001b[32m     82\u001b[39m \u001b[38;5;28mlen\u001b[39m(mp_list), \u001b[38;5;28mlen\u001b[39m(mp_details), \u001b[38;5;28mlen\u001b[39m(clubs_list)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mdownload_mps_and_clubs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     39\u001b[39m clubs_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAPI_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/sejm/term\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTERM\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/clubs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m clubs_path = RAW_DIR / \u001b[33m\"\u001b[39m\u001b[33mclubs\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mclubs.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmake_client\u001b[49m() \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# MP list + clubs list\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fetch_json_to_file(client, mp_list_url, mp_list_path)\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fetch_json_to_file(client, clubs_url, clubs_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'make_client' is not defined"
     ]
    }
   ],
   "source": [
    "async def fetch_json_to_file(client: httpx.AsyncClient, url: str, path: Path, *, accept_404: bool = False) -> FetchResult:\n",
    "    if path.exists() and path.stat().st_size > 0:\n",
    "        return FetchResult(url=url, status_code=200, ok=True, path=path)\n",
    "\n",
    "    async with SEM:\n",
    "        try:\n",
    "            r = await request_with_retries(client, \"GET\", url, headers={\"Accept\": \"application/json\"}, accept_404=accept_404)\n",
    "            if accept_404 and r.status_code == 404:\n",
    "                await write_text(path, \"\")  # marker\n",
    "                return FetchResult(url=url, status_code=404, ok=False, path=path, error=\"404\")\n",
    "\n",
    "            obj = r.json()\n",
    "            await write_json(path, obj)\n",
    "            return FetchResult(url=url, status_code=r.status_code, ok=True, path=path)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"JSON fetch failed: %s\", url)\n",
    "            return FetchResult(url=url, status_code=0, ok=False, path=path, error=str(e))\n",
    "\n",
    "async def fetch_text_to_file(client: httpx.AsyncClient, url: str, path: Path, *, accept: str = \"text/html\", accept_404: bool = False) -> FetchResult:\n",
    "    if path.exists() and path.stat().st_size > 0:\n",
    "        return FetchResult(url=url, status_code=200, ok=True, path=path)\n",
    "\n",
    "    async with SEM:\n",
    "        try:\n",
    "            r = await request_with_retries(client, \"GET\", url, headers={\"Accept\": accept}, accept_404=accept_404)\n",
    "            if accept_404 and r.status_code == 404:\n",
    "                await write_text(path, \"\")  # marker\n",
    "                return FetchResult(url=url, status_code=404, ok=False, path=path, error=\"404\")\n",
    "            await write_text(path, r.text)\n",
    "            return FetchResult(url=url, status_code=r.status_code, ok=True, path=path)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"TEXT fetch failed: %s\", url)\n",
    "            return FetchResult(url=url, status_code=0, ok=False, path=path, error=str(e))\n",
    "            \n",
    "async def download_mps_and_clubs() -> tuple[list[dict[str, Any]], dict[int, dict[str, Any]], list[dict[str, Any]]]:\n",
    "    mp_list_url = f\"{API_ROOT}/sejm/term{TERM}/MP\"\n",
    "    mp_list_path = RAW_DIR / \"mp\" / \"mp_list.json\"\n",
    "\n",
    "    clubs_url = f\"{API_ROOT}/sejm/term{TERM}/clubs\"\n",
    "    clubs_path = RAW_DIR / \"clubs\" / \"clubs.json\"\n",
    "\n",
    "    async with make_client() as client:\n",
    "        # MP list + clubs list\n",
    "        await fetch_json_to_file(client, mp_list_url, mp_list_path)\n",
    "        await fetch_json_to_file(client, clubs_url, clubs_path)\n",
    "\n",
    "        mp_list = json.loads(mp_list_path.read_text(encoding=\"utf-8\"))\n",
    "        clubs_list = json.loads(clubs_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        # MP details\n",
    "        mp_details: dict[int, dict[str, Any]] = {}\n",
    "        jobs = []\n",
    "        for mp in mp_list:\n",
    "            mp_id = safe_int(mp.get(\"id\") or mp.get(\"ID\") or mp.get(\"mpId\"))\n",
    "            if not mp_id:\n",
    "                continue\n",
    "            url = f\"{API_ROOT}/sejm/term{TERM}/MP/{mp_id}\"\n",
    "            path = RAW_DIR / \"mp\" / \"details\" / f\"{mp_id}.json\"\n",
    "            jobs.append((mp_id, url, path))\n",
    "\n",
    "        logger.info(\"MP details to fetch: %d\", len(jobs))\n",
    "\n",
    "        results = []\n",
    "        with tqdm(total=len(jobs), desc=\"MP details\") as pbar:\n",
    "            tasks = [fetch_json_to_file(client, url, path) for _, url, path in jobs]\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                res = await coro\n",
    "                results.append(res)\n",
    "                pbar.update(1)\n",
    "\n",
    "        # read details back (fast enough)\n",
    "        for mp_id, _, path in jobs:\n",
    "            if path.exists() and path.stat().st_size > 0:\n",
    "                try:\n",
    "                    mp_details[mp_id] = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        return mp_list, mp_details, clubs_list\n",
    "\n",
    "mp_list, mp_details, clubs_list = await download_mps_and_clubs()\n",
    "len(mp_list), len(mp_details), len(clubs_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d2f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(path: Path, rows: Iterable[dict[str, Any]], fieldnames: list[str]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k, \"\") for k in fieldnames})\n",
    "\n",
    "# DIM: MPs\n",
    "mp_dim_rows = []\n",
    "for mp in mp_list:\n",
    "    mp_id = safe_int(mp.get(\"id\") or mp.get(\"ID\") or mp.get(\"mpId\"))\n",
    "    if not mp_id:\n",
    "        continue\n",
    "    det = mp_details.get(mp_id, {})\n",
    "    mp_dim_rows.append({\n",
    "        \"mp_id\": mp_id,\n",
    "        \"name\": mp.get(\"firstLastName\") or mp.get(\"name\") or det.get(\"firstLastName\") or \"\",\n",
    "        \"club\": mp.get(\"club\") or det.get(\"club\") or \"\",\n",
    "        \"districtName\": det.get(\"districtName\") or \"\",\n",
    "        \"voivodeship\": det.get(\"voivodeship\") or \"\",\n",
    "        \"birthDate\": det.get(\"birthDate\") or \"\",\n",
    "        \"birthPlace\": det.get(\"birthPlace\") or \"\",\n",
    "        \"educationLevel\": det.get(\"educationLevel\") or det.get(\"education\") or \"\",\n",
    "        \"profession\": det.get(\"profession\") or \"\",\n",
    "    })\n",
    "\n",
    "mp_dim_path = TABLES_DIR / \"dim_mp.csv\"\n",
    "write_csv(mp_dim_path, mp_dim_rows, list(mp_dim_rows[0].keys()) if mp_dim_rows else [\"mp_id\"])\n",
    "mp_dim_path\n",
    "\n",
    "# DIM: Clubs\n",
    "club_rows = []\n",
    "for c in clubs_list:\n",
    "    club_rows.append({\n",
    "        \"club_id\": c.get(\"id\") or \"\",\n",
    "        \"name\": c.get(\"name\") or \"\",\n",
    "        \"membersCount\": c.get(\"membersCount\") or \"\",\n",
    "    })\n",
    "\n",
    "club_dim_path = TABLES_DIR / \"dim_club.csv\"\n",
    "write_csv(club_dim_path, club_rows, [\"club_id\", \"name\", \"membersCount\"])\n",
    "club_dim_path\n",
    "\n",
    "# Mapy pomocnicze do katalogowania\n",
    "mp_id_to_name = {r[\"mp_id\"]: r[\"name\"] for r in mp_dim_rows}\n",
    "mp_id_to_slug = {mp_id: slugify(name) for mp_id, name in mp_id_to_name.items()}\n",
    "len(mp_id_to_slug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d797c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_proceedings_and_daily_indexes() -> list[dict[str, Any]]:\n",
    "    proceedings_url = f\"{API_ROOT}/sejm/term{TERM}/proceedings\"\n",
    "    proceedings_path = RAW_DIR / \"proceedings\" / \"proceedings.json\"\n",
    "\n",
    "    async with make_client() as client:\n",
    "        await fetch_json_to_file(client, proceedings_url, proceedings_path)\n",
    "        proceedings = json.loads(proceedings_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        # Zapisz też szczegóły per posiedzenie (jeśli endpoint istnieje)\n",
    "        # (API praktycznie to ma; gdyby nie, kod się nie wywali – tylko pominie 404)\n",
    "        unique_nums = []\n",
    "        for p in proceedings:\n",
    "            n = safe_int(p.get(\"num\") or p.get(\"number\") or p.get(\"proceedingNum\") or p.get(\"sitting\") or p.get(\"id\"))\n",
    "            if n is not None:\n",
    "                unique_nums.append(n)\n",
    "        unique_nums = sorted(set(unique_nums))\n",
    "\n",
    "        logger.info(\"Proceedings discovered: %d\", len(unique_nums))\n",
    "\n",
    "        detail_tasks = []\n",
    "        for n in unique_nums:\n",
    "            url = f\"{API_ROOT}/sejm/term{TERM}/proceedings/{n}\"\n",
    "            path = RAW_DIR / \"proceedings\" / \"details\" / f\"{n}.json\"\n",
    "            detail_tasks.append((n, url, path))\n",
    "\n",
    "        with tqdm(total=len(detail_tasks), desc=\"Proceeding details\") as pbar:\n",
    "            tasks = [fetch_json_to_file(client, url, path, accept_404=True) for _, url, path in detail_tasks]\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                _ = await coro\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Zbuduj listę (proceedingNum, date) do indeksów transcripts\n",
    "        day_pairs: list[tuple[int, str]] = []\n",
    "\n",
    "        def extract_dates(obj: Any) -> list[str]:\n",
    "            # szukamy listy stringów YYYY-MM-DD w typowych polach, albo rekurencyjnie\n",
    "            found = set()\n",
    "\n",
    "            def walk(x: Any) -> None:\n",
    "                if isinstance(x, str) and re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}\", x):\n",
    "                    found.add(x)\n",
    "                elif isinstance(x, list):\n",
    "                    for it in x:\n",
    "                        walk(it)\n",
    "                elif isinstance(x, dict):\n",
    "                    for v in x.values():\n",
    "                        walk(v)\n",
    "\n",
    "            # szybkie ścieżki\n",
    "            if isinstance(obj, dict):\n",
    "                for key in (\"dates\", \"days\", \"sittingDays\", \"sessionDays\"):\n",
    "                    if key in obj:\n",
    "                        walk(obj[key])\n",
    "            walk(obj)\n",
    "            return sorted(found)\n",
    "\n",
    "        for n in unique_nums:\n",
    "            # spróbuj wziąć daty z detali; jeśli brak, spróbuj z listy proceedings\n",
    "            det_path = RAW_DIR / \"proceedings\" / \"details\" / f\"{n}.json\"\n",
    "            dates = []\n",
    "            if det_path.exists() and det_path.stat().st_size > 0:\n",
    "                try:\n",
    "                    det = json.loads(det_path.read_text(encoding=\"utf-8\"))\n",
    "                    dates = extract_dates(det)\n",
    "                except Exception:\n",
    "                    dates = []\n",
    "\n",
    "            if not dates:\n",
    "                # fallback: poszukaj w obiekcie z listy proceedings\n",
    "                for p in proceedings:\n",
    "                    pn = safe_int(p.get(\"num\") or p.get(\"number\") or p.get(\"proceedingNum\") or p.get(\"sitting\") or p.get(\"id\"))\n",
    "                    if pn == n:\n",
    "                        dates = extract_dates(p)\n",
    "                        break\n",
    "\n",
    "            for d in dates:\n",
    "                day_pairs.append((n, d))\n",
    "\n",
    "        day_pairs = sorted(set(day_pairs))\n",
    "        logger.info(\"Proceeding-day pairs (for transcript indexes): %d\", len(day_pairs))\n",
    "\n",
    "        # Pobierz indeks stenogramów per dzień\n",
    "        idx_tasks = []\n",
    "        for n, d in day_pairs:\n",
    "            url = f\"{API_ROOT}/sejm/term{TERM}/proceedings/{n}/{d}/transcripts\"\n",
    "            path = RAW_DIR / \"transcripts\" / \"index_by_day\" / f\"p{n}\" / f\"d{d}.json\"\n",
    "            idx_tasks.append((n, d, url, path))\n",
    "\n",
    "        ok_pairs = []\n",
    "        with tqdm(total=len(idx_tasks), desc=\"Transcript indexes\") as pbar:\n",
    "            tasks = [fetch_json_to_file(client, url, path, accept_404=True) for _, _, url, path in idx_tasks]\n",
    "            for (n, d, _, path), coro in zip(idx_tasks, asyncio.as_completed(tasks)):\n",
    "                res = await coro\n",
    "                if res.ok:\n",
    "                    ok_pairs.append((n, d))\n",
    "                pbar.update(1)\n",
    "\n",
    "        return proceedings\n",
    "\n",
    "proceedings = await download_proceedings_and_daily_indexes()\n",
    "len(proceedings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_transcript_indexes() -> Iterable[Path]:\n",
    "    base = RAW_DIR / \"transcripts\" / \"index_by_day\"\n",
    "    if not base.exists():\n",
    "        return []\n",
    "    return base.rglob(\"d*.json\")\n",
    "\n",
    "statement_rows = []\n",
    "html_jobs = []  # (url, path, mp_id or None)\n",
    "\n",
    "for idx_path in iter_transcript_indexes():\n",
    "    try:\n",
    "        obj = json.loads(idx_path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    proceeding_num = safe_int(obj.get(\"proceedingNum\"))\n",
    "    date = obj.get(\"date\")\n",
    "    statements = obj.get(\"statements\") or []\n",
    "\n",
    "    if not proceeding_num or not date:\n",
    "        continue\n",
    "\n",
    "    for st in statements:\n",
    "        st_num = st.get(\"num\")\n",
    "        mp_id = safe_int(st.get(\"memberID\"))\n",
    "        name = st.get(\"name\") or \"\"\n",
    "        function = st.get(\"function\") or \"\"\n",
    "        unspoken = st.get(\"unspoken\")\n",
    "\n",
    "        statement_rows.append({\n",
    "            \"term\": TERM,\n",
    "            \"proceedingNum\": proceeding_num,\n",
    "            \"date\": date,\n",
    "            \"statementNum\": st_num,\n",
    "            \"memberID\": mp_id if mp_id is not None else \"\",\n",
    "            \"name\": name,\n",
    "            \"function\": function,\n",
    "            \"startDateTime\": st.get(\"startDateTime\") or \"\",\n",
    "            \"endDateTime\": st.get(\"endDateTime\") or \"\",\n",
    "            \"rapporteur\": st.get(\"rapporteur\") if st.get(\"rapporteur\") is not None else \"\",\n",
    "            \"secretary\": st.get(\"secretary\") if st.get(\"secretary\") is not None else \"\",\n",
    "            \"unspoken\": unspoken if unspoken is not None else \"\",\n",
    "        })\n",
    "\n",
    "        # docelowy plik HTML (bez preprocessingu)\n",
    "        st_num_str = str(st_num)\n",
    "        url = f\"{API_ROOT}/sejm/term{TERM}/proceedings/{proceeding_num}/{date}/transcripts/{st_num_str}\"\n",
    "\n",
    "        if mp_id and mp_id > 0:\n",
    "            mp_folder = f\"{mp_id:03d}-{mp_id_to_slug.get(mp_id, slugify(name))}\"\n",
    "            path = RAW_DIR / \"transcripts\" / \"html_by_mp\" / mp_folder / f\"p{proceeding_num}\" / f\"d{date}\" / f\"s{st_num_str}.html\"\n",
    "        else:\n",
    "            path = RAW_DIR / \"transcripts\" / \"html_non_mp\" / f\"p{proceeding_num}\" / f\"d{date}\" / f\"s{st_num_str}.html\"\n",
    "\n",
    "        html_jobs.append((url, path))\n",
    "\n",
    "# Zapisz index CSV\n",
    "fact_transcripts_path = TABLES_DIR / \"fact_transcript_statement_index.csv\"\n",
    "write_csv(\n",
    "    fact_transcripts_path,\n",
    "    statement_rows,\n",
    "    [\"term\",\"proceedingNum\",\"date\",\"statementNum\",\"memberID\",\"name\",\"function\",\"startDateTime\",\"endDateTime\",\"rapporteur\",\"secretary\",\"unspoken\"]\n",
    ")\n",
    "fact_transcripts_path, len(statement_rows), len(html_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab065ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_many_html(jobs: list[tuple[str, Path]], desc: str) -> tuple[int, int]:\n",
    "    ok = 0\n",
    "    fail = 0\n",
    "    async with make_client() as client:\n",
    "        with tqdm(total=len(jobs), desc=desc) as pbar:\n",
    "            tasks = [fetch_text_to_file(client, url, path, accept=\"text/html\", accept_404=True) for url, path in jobs]\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                res = await coro\n",
    "                if res.ok:\n",
    "                    ok += 1\n",
    "                else:\n",
    "                    fail += 1\n",
    "                pbar.update(1)\n",
    "    return ok, fail\n",
    "\n",
    "if DOWNLOAD_TRANSCRIPTS:\n",
    "    ok, fail = await download_many_html(html_jobs, desc=\"Downloading transcripts (HTML)\")\n",
    "    print(\"OK:\", ok, \"FAIL:\", fail)\n",
    "else:\n",
    "    print(\"DOWNLOAD_TRANSCRIPTS=False, pomijam.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def paginated_list(\n",
    "    client: httpx.AsyncClient,\n",
    "    url: str,\n",
    "    page_dir: Path,\n",
    "    *,\n",
    "    limit: int = PAGE_LIMIT,\n",
    "    hard_stop: int = 1_000_000,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Pobiera wszystkie strony listy typu:\n",
    "    GET ...?offset=0&limit=...\n",
    "    Zwraca spłaszczoną listę rekordów.\n",
    "    \"\"\"\n",
    "    all_items: list[dict[str, Any]] = []\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        if offset >= hard_stop:\n",
    "            logger.warning(\"Hard stop reached for %s\", url)\n",
    "            break\n",
    "\n",
    "        page_path = page_dir / f\"offset={offset}_limit={limit}.json\"\n",
    "        page_url = url\n",
    "\n",
    "        # cache\n",
    "        if page_path.exists() and page_path.stat().st_size > 0:\n",
    "            items = json.loads(page_path.read_text(encoding=\"utf-8\"))\n",
    "        else:\n",
    "            r = await request_with_retries(\n",
    "                client,\n",
    "                \"GET\",\n",
    "                page_url,\n",
    "                headers={\"Accept\": \"application/json\"},\n",
    "                params={\"offset\": offset, \"limit\": limit},\n",
    "            )\n",
    "            items = r.json()\n",
    "            await write_json(page_path, items)\n",
    "\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        all_items.extend(items)\n",
    "\n",
    "        # jeśli API zwraca mniej niż limit, to zwykle koniec\n",
    "        if isinstance(items, list) and len(items) < limit:\n",
    "            break\n",
    "\n",
    "        offset += limit\n",
    "\n",
    "    return all_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b825c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_interpellations() -> None:\n",
    "    base_list_url = f\"{API_ROOT}/sejm/term{TERM}/interpellations\"\n",
    "    page_dir = RAW_DIR / \"interpellations\" / \"pages\"\n",
    "\n",
    "    async with make_client() as client:\n",
    "        items = await paginated_list(client, base_list_url, page_dir, limit=PAGE_LIMIT)\n",
    "        logger.info(\"Interpellations list items: %d\", len(items))\n",
    "\n",
    "        nums = []\n",
    "        for it in items:\n",
    "            n = safe_int(it.get(\"num\"))\n",
    "            if n is not None:\n",
    "                nums.append(n)\n",
    "        nums = sorted(set(nums))\n",
    "        logger.info(\"Interpellations unique nums: %d\", len(nums))\n",
    "\n",
    "        # details\n",
    "        detail_jobs = []\n",
    "        for n in nums:\n",
    "            url = f\"{API_ROOT}/sejm/term{TERM}/interpellations/{n}\"\n",
    "            path = RAW_DIR / \"interpellations\" / \"by_num\" / f\"{n}\" / \"details.json\"\n",
    "            detail_jobs.append((n, url, path))\n",
    "\n",
    "        with tqdm(total=len(detail_jobs), desc=\"Interpellations details\") as pbar:\n",
    "            tasks = [fetch_json_to_file(client, url, path, accept_404=True) for _, url, path in detail_jobs]\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                _ = await coro\n",
    "                pbar.update(1)\n",
    "\n",
    "        # bodies + replies + attachments\n",
    "        body_jobs = []\n",
    "        reply_jobs = []\n",
    "        attach_jobs = []  # (url, path)\n",
    "\n",
    "        author_ptr_rows = []  # pointer files per author for quick folder iteration\n",
    "        for n in nums:\n",
    "            det_path = RAW_DIR / \"interpellations\" / \"by_num\" / f\"{n}\" / \"details.json\"\n",
    "            if not det_path.exists() or det_path.stat().st_size == 0:\n",
    "                continue\n",
    "            try:\n",
    "                det = json.loads(det_path.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # authors (field \"from\" bywa listą stringów z numerami legitymacji)\n",
    "            authors = det.get(\"from\") or []\n",
    "            author_ids = []\n",
    "            for a in authors:\n",
    "                ai = safe_int(a)\n",
    "                if ai is not None:\n",
    "                    author_ids.append(ai)\n",
    "\n",
    "            for ai in author_ids:\n",
    "                ptr_dir = RAW_DIR / \"interpellations\" / \"by_author_ptr\" / f\"{ai:03d}-{mp_id_to_slug.get(ai,'unknown')}\"\n",
    "                ptr_path = ptr_dir / f\"i{n}.json\"\n",
    "                author_ptr_rows.append({\"mp_id\": ai, \"num\": n, \"ptr_path\": str(ptr_path.relative_to(RAW_DIR))})\n",
    "                if not ptr_path.exists():\n",
    "                    await write_json(ptr_path, {\"type\": \"interpellation\", \"num\": n, \"target\": str(det_path.parent)})\n",
    "\n",
    "            # body html\n",
    "            body_url = f\"{API_ROOT}/sejm/term{TERM}/interpellations/{n}/body\"\n",
    "            body_path = RAW_DIR / \"interpellations\" / \"by_num\" / f\"{n}\" / \"body.html\"\n",
    "            body_jobs.append((body_url, body_path))\n",
    "\n",
    "            # replies\n",
    "            for rep in det.get(\"replies\") or []:\n",
    "                key = rep.get(\"key\")\n",
    "                if not key:\n",
    "                    continue\n",
    "                rep_url = f\"{API_ROOT}/sejm/term{TERM}/interpellations/{n}/reply/{key}/body\"\n",
    "                rep_path = RAW_DIR / \"interpellations\" / \"by_num\" / f\"{n}\" / \"replies\" / f\"{key}.html\"\n",
    "                reply_jobs.append((rep_url, rep_path))\n",
    "\n",
    "                # attachments in reply\n",
    "                if DOWNLOAD_ATTACHMENTS:\n",
    "                    for att in rep.get(\"attachments\") or []:\n",
    "                        aurl = att.get(\"URL\")\n",
    "                        aname = att.get(\"name\") or \"attachment.bin\"\n",
    "                        if aurl:\n",
    "                            apath = RAW_DIR / \"interpellations\" / \"by_num\" / f\"{n}\" / \"attachments\" / f\"{key}\" / aname\n",
    "                            attach_jobs.append((aurl, apath))\n",
    "\n",
    "        # zapisz mały indeks pointerów autorów\n",
    "        ptr_index_path = TABLES_DIR / \"ptr_interpellations_by_author.csv\"\n",
    "        if author_ptr_rows:\n",
    "            write_csv(ptr_index_path, author_ptr_rows, [\"mp_id\", \"num\", \"ptr_path\"])\n",
    "\n",
    "        # pobieranie HTML\n",
    "        ok1, fail1 = await download_many_html(body_jobs, \"Interpellations bodies (HTML)\")\n",
    "        ok2, fail2 = await download_many_html(reply_jobs, \"Interpellations replies (HTML)\")\n",
    "        logger.info(\"Interpellations body OK=%d FAIL=%d; reply OK=%d FAIL=%d\", ok1, fail1, ok2, fail2)\n",
    "\n",
    "        # pobieranie załączników (streaming)\n",
    "        async def download_attachment(url: str, path: Path) -> FetchResult:\n",
    "            if path.exists() and path.stat().st_size > 0:\n",
    "                return FetchResult(url=url, status_code=200, ok=True, path=path)\n",
    "\n",
    "            async with SEM:\n",
    "                try:\n",
    "                    async with client.stream(\"GET\", url, headers={\"Accept\": \"*/*\"}) as r:\n",
    "                        if r.status_code == 404:\n",
    "                            await write_bytes(path, b\"\")  # marker\n",
    "                            return FetchResult(url=url, status_code=404, ok=False, path=path, error=\"404\")\n",
    "                        r.raise_for_status()\n",
    "                        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        # zapis strumieniowy\n",
    "                        with path.open(\"wb\") as f:\n",
    "                            async for chunk in r.aiter_bytes():\n",
    "                                f.write(chunk)\n",
    "                    return FetchResult(url=url, status_code=200, ok=True, path=path)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Attachment failed: %s\", url)\n",
    "                    return FetchResult(url=url, status_code=0, ok=False, path=path, error=str(e))\n",
    "\n",
    "        if DOWNLOAD_ATTACHMENTS and attach_jobs:\n",
    "            ok_a = 0\n",
    "            fail_a = 0\n",
    "            with tqdm(total=len(attach_jobs), desc=\"Interpellations attachments\") as pbar:\n",
    "                tasks = [download_attachment(url, path) for url, path in attach_jobs]\n",
    "                for coro in asyncio.as_completed(tasks):\n",
    "                    res = await coro\n",
    "                    if res.ok:\n",
    "                        ok_a += 1\n",
    "                    else:\n",
    "                        fail_a += 1\n",
    "                    pbar.update(1)\n",
    "            logger.info(\"Interpellations attachments OK=%d FAIL=%d\", ok_a, fail_a)\n",
    "\n",
    "if DOWNLOAD_INTERPELLATIONS:\n",
    "    await download_interpellations()\n",
    "else:\n",
    "    print(\"DOWNLOAD_INTERPELLATIONS=False, pomijam.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0751f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_written_questions() -> None:\n",
    "    base_list_url = f\"{API_ROOT}/sejm/term{TERM}/writtenQuestions\"\n",
    "    page_dir = RAW_DIR / \"writtenQuestions\" / \"pages\"\n",
    "\n",
    "    async with make_client() as client:\n",
    "        items = await paginated_list(client, base_list_url, page_dir, limit=PAGE_LIMIT)\n",
    "        logger.info(\"writtenQuestions list items: %d\", len(items))\n",
    "\n",
    "        nums = []\n",
    "        for it in items:\n",
    "            n = safe_int(it.get(\"num\"))\n",
    "            if n is not None:\n",
    "                nums.append(n)\n",
    "        nums = sorted(set(nums))\n",
    "        logger.info(\"writtenQuestions unique nums: %d\", len(nums))\n",
    "\n",
    "        # details\n",
    "        detail_jobs = []\n",
    "        for n in nums:\n",
    "            url = f\"{API_ROOT}/sejm/term{TERM}/writtenQuestions/{n}\"\n",
    "            path = RAW_DIR / \"writtenQuestions\" / \"by_num\" / f\"{n}\" / \"details.json\"\n",
    "            detail_jobs.append((n, url, path))\n",
    "\n",
    "        with tqdm(total=len(detail_jobs), desc=\"writtenQuestions details\") as pbar:\n",
    "            tasks = [fetch_json_to_file(client, url, path, accept_404=True) for _, url, path in detail_jobs]\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                _ = await coro\n",
    "                pbar.update(1)\n",
    "\n",
    "        # bodies + replies + attachments\n",
    "        body_jobs = []\n",
    "        reply_jobs = []\n",
    "        attach_jobs = []\n",
    "\n",
    "        author_ptr_rows = []\n",
    "        for n in nums:\n",
    "            det_path = RAW_DIR / \"writtenQuestions\" / \"by_num\" / f\"{n}\" / \"details.json\"\n",
    "            if not det_path.exists() or det_path.stat().st_size == 0:\n",
    "                continue\n",
    "            try:\n",
    "                det = json.loads(det_path.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            authors = det.get(\"from\") or []\n",
    "            author_ids = []\n",
    "            for a in authors:\n",
    "                ai = safe_int(a)\n",
    "                if ai is not None:\n",
    "                    author_ids.append(ai)\n",
    "\n",
    "            for ai in author_ids:\n",
    "                ptr_dir = RAW_DIR / \"writtenQuestions\" / \"by_author_ptr\" / f\"{ai:03d}-{mp_id_to_slug.get(ai,'unknown')}\"\n",
    "                ptr_path = ptr_dir / f\"q{n}.json\"\n",
    "                author_ptr_rows.append({\"mp_id\": ai, \"num\": n, \"ptr_path\": str(ptr_path.relative_to(RAW_DIR))})\n",
    "                if not ptr_path.exists():\n",
    "                    await write_json(ptr_path, {\"type\": \"writtenQuestion\", \"num\": n, \"target\": str(det_path.parent)})\n",
    "\n",
    "            body_url = f\"{API_ROOT}/sejm/term{TERM}/writtenQuestions/{n}/body\"\n",
    "            body_path = RAW_DIR / \"writtenQuestions\" / \"by_num\" / f\"{n}\" / \"body.html\"\n",
    "            body_jobs.append((body_url, body_path))\n",
    "\n",
    "            for rep in det.get(\"replies\") or []:\n",
    "                key = rep.get(\"key\")\n",
    "                if not key:\n",
    "                    continue\n",
    "                rep_url = f\"{API_ROOT}/sejm/term{TERM}/writtenQuestions/{n}/reply/{key}/body\"\n",
    "                rep_path = RAW_DIR / \"writtenQuestions\" / \"by_num\" / f\"{n}\" / \"replies\" / f\"{key}.html\"\n",
    "                reply_jobs.append((rep_url, rep_path))\n",
    "\n",
    "                if DOWNLOAD_ATTACHMENTS:\n",
    "                    for att in rep.get(\"attachments\") or []:\n",
    "                        aurl = att.get(\"URL\")\n",
    "                        aname = att.get(\"name\") or \"attachment.bin\"\n",
    "                        if aurl:\n",
    "                            apath = RAW_DIR / \"writtenQuestions\" / \"by_num\" / f\"{n}\" / \"attachments\" / f\"{key}\" / aname\n",
    "                            attach_jobs.append((aurl, apath))\n",
    "\n",
    "        ptr_index_path = TABLES_DIR / \"ptr_writtenQuestions_by_author.csv\"\n",
    "        if author_ptr_rows:\n",
    "            write_csv(ptr_index_path, author_ptr_rows, [\"mp_id\", \"num\", \"ptr_path\"])\n",
    "\n",
    "        ok1, fail1 = await download_many_html(body_jobs, \"writtenQuestions bodies (HTML)\")\n",
    "        ok2, fail2 = await download_many_html(reply_jobs, \"writtenQuestions replies (HTML)\")\n",
    "        logger.info(\"writtenQuestions body OK=%d FAIL=%d; reply OK=%d FAIL=%d\", ok1, fail1, ok2, fail2)\n",
    "\n",
    "        async def download_attachment(url: str, path: Path) -> FetchResult:\n",
    "            if path.exists() and path.stat().st_size > 0:\n",
    "                return FetchResult(url=url, status_code=200, ok=True, path=path)\n",
    "\n",
    "            async with SEM:\n",
    "                try:\n",
    "                    async with client.stream(\"GET\", url, headers={\"Accept\": \"*/*\"}) as r:\n",
    "                        if r.status_code == 404:\n",
    "                            await write_bytes(path, b\"\")\n",
    "                            return FetchResult(url=url, status_code=404, ok=False, path=path, error=\"404\")\n",
    "                        r.raise_for_status()\n",
    "                        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        with path.open(\"wb\") as f:\n",
    "                            async for chunk in r.aiter_bytes():\n",
    "                                f.write(chunk)\n",
    "                    return FetchResult(url=url, status_code=200, ok=True, path=path)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"Attachment failed: %s\", url)\n",
    "                    return FetchResult(url=url, status_code=0, ok=False, path=path, error=str(e))\n",
    "\n",
    "        if DOWNLOAD_ATTACHMENTS and attach_jobs:\n",
    "            ok_a = 0\n",
    "            fail_a = 0\n",
    "            with tqdm(total=len(attach_jobs), desc=\"writtenQuestions attachments\") as pbar:\n",
    "                tasks = [download_attachment(url, path) for url, path in attach_jobs]\n",
    "                for coro in asyncio.as_completed(tasks):\n",
    "                    res = await coro\n",
    "                    if res.ok:\n",
    "                        ok_a += 1\n",
    "                    else:\n",
    "                        fail_a += 1\n",
    "                    pbar.update(1)\n",
    "            logger.info(\"writtenQuestions attachments OK=%d FAIL=%d\", ok_a, fail_a)\n",
    "\n",
    "if DOWNLOAD_WRITTEN_QUESTIONS:\n",
    "    await download_written_questions()\n",
    "else:\n",
    "    print(\"DOWNLOAD_WRITTEN_QUESTIONS=False, pomijam.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f11a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_committees() -> None:\n",
    "    async with make_client() as client:\n",
    "        # lista komisji\n",
    "        committees_url = f\"{API_ROOT}/sejm/term{TERM}/committees\"\n",
    "        committees_path = RAW_DIR / \"committees\" / \"committees.json\"\n",
    "        await fetch_json_to_file(client, committees_url, committees_path)\n",
    "        committees = json.loads(committees_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        # per komisja: lista posiedzeń\n",
    "        sitting_jobs = []  # (code, sitting_num)\n",
    "        for c in committees:\n",
    "            code = c.get(\"code\") or c.get(\"id\") or c.get(\"committeeCode\")\n",
    "            if not code:\n",
    "                continue\n",
    "            url = f\"{API_ROOT}/sejm/term{TERM}/committees/{code}/sittings\"\n",
    "            path = RAW_DIR / \"committees\" / \"sittings\" / f\"{code}_sittings.json\"\n",
    "            await fetch_json_to_file(client, url, path, accept_404=True)\n",
    "            if not path.exists() or path.stat().st_size == 0:\n",
    "                continue\n",
    "            try:\n",
    "                sittings = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "            for s in sittings:\n",
    "                sn = safe_int(s.get(\"num\") or s.get(\"number\") or s.get(\"id\"))\n",
    "                if sn is not None:\n",
    "                    sitting_jobs.append((code, sn))\n",
    "\n",
    "        sitting_jobs = sorted(set(sitting_jobs))\n",
    "        logger.info(\"Committee sittings to fetch: %d\", len(sitting_jobs))\n",
    "\n",
    "        # pobierz HTML/PDF zapis posiedzenia\n",
    "        html_jobs = []\n",
    "        pdf_jobs = []\n",
    "        for code, sn in sitting_jobs:\n",
    "            html_url = f\"{API_ROOT}/sejm/term{TERM}/committees/{code}/sittings/{sn}/html\"\n",
    "            pdf_url = f\"{API_ROOT}/sejm/term{TERM}/committees/{code}/sittings/{sn}/pdf\"\n",
    "            html_path = RAW_DIR / \"committees\" / \"sittings\" / code / f\"{sn}\" / \"sitting.html\"\n",
    "            pdf_path = RAW_DIR / \"committees\" / \"sittings\" / code / f\"{sn}\" / \"sitting.pdf\"\n",
    "            html_jobs.append((html_url, html_path))\n",
    "            pdf_jobs.append((pdf_url, pdf_path))\n",
    "\n",
    "        ok_h, fail_h = await download_many_html(html_jobs, \"Committees sittings (HTML)\")\n",
    "\n",
    "        # pdf streaming\n",
    "        async def download_pdf(url: str, path: Path) -> FetchResult:\n",
    "            if path.exists() and path.stat().st_size > 0:\n",
    "                return FetchResult(url=url, status_code=200, ok=True, path=path)\n",
    "            async with SEM:\n",
    "                try:\n",
    "                    async with client.stream(\"GET\", url, headers={\"Accept\": \"application/pdf\"}) as r:\n",
    "                        if r.status_code == 404:\n",
    "                            await write_bytes(path, b\"\")\n",
    "                            return FetchResult(url=url, status_code=404, ok=False, path=path, error=\"404\")\n",
    "                        r.raise_for_status()\n",
    "                        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        with path.open(\"wb\") as f:\n",
    "                            async for chunk in r.aiter_bytes():\n",
    "                                f.write(chunk)\n",
    "                    return FetchResult(url=url, status_code=200, ok=True, path=path)\n",
    "                except Exception as e:\n",
    "                    logger.exception(\"PDF failed: %s\", url)\n",
    "                    return FetchResult(url=url, status_code=0, ok=False, path=path, error=str(e))\n",
    "\n",
    "        ok_p = 0\n",
    "        fail_p = 0\n",
    "        with tqdm(total=len(pdf_jobs), desc=\"Committees sittings (PDF)\") as pbar:\n",
    "            tasks = [download_pdf(url, path) for url, path in pdf_jobs]\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                res = await coro\n",
    "                if res.ok:\n",
    "                    ok_p += 1\n",
    "                else:\n",
    "                    fail_p += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "        logger.info(\"Committees: HTML OK=%d FAIL=%d; PDF OK=%d FAIL=%d\", ok_h, fail_h, ok_p, fail_p)\n",
    "\n",
    "if DOWNLOAD_COMMITTEE_SITTINGS:\n",
    "    await download_committees()\n",
    "else:\n",
    "    print(\"DOWNLOAD_COMMITTEE_SITTINGS=False, pomijam.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(p: Path, pattern: str) -> int:\n",
    "    return sum(1 for _ in p.rglob(pattern))\n",
    "\n",
    "print(\"MP details:\", count_files(RAW_DIR / \"mp\" / \"details\", \"*.json\"))\n",
    "print(\"Transcript indexes:\", count_files(RAW_DIR / \"transcripts\" / \"index_by_day\", \"*.json\"))\n",
    "print(\"Transcript HTML (MP):\", count_files(RAW_DIR / \"transcripts\" / \"html_by_mp\", \"*.html\"))\n",
    "print(\"Interpellations bodies:\", count_files(RAW_DIR / \"interpellations\" / \"by_num\", \"body.html\"))\n",
    "print(\"writtenQuestions bodies:\", count_files(RAW_DIR / \"writtenQuestions\" / \"by_num\", \"body.html\"))\n",
    "\n",
    "print(\"Tables:\", list(TABLES_DIR.glob(\"*.csv\")))\n",
    "print(\"Log:\", LOG_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
